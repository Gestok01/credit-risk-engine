Credit Risk Engine â€“ Audit-Ready ML Inference Service
A production-oriented Credit Risk Prediction API built to demonstrate how machine learning models are served, explained, monitored, and audited in real-world systems â€” not just trained in notebooks.
This project focuses on the engineering side of ML, covering inference pipelines, explainability, drift monitoring, containerization, and cloud deployment.

ğŸ” Why this project exists
Most ML projects stop at â€œtrained a model, got 92% accuracy.â€
This one starts after that point.
The goal was to answer:


How is a trained model served safely?


How do we explain its decisions?


How do we detect when it starts lying (data drift)?


How do we make it auditable and deployment-ready?



ğŸ§  Model Overview


Type: Binary classification (default risk)


Algorithm: Logistic Regression (scikit-learn pipeline)


Target: Probability of default


Output: Risk band + decision


The trained pipeline is serialized and loaded directly at runtime.

ğŸ—ï¸ System Architecture
Client
  â”‚
  â”‚ JSON request
  â–¼
FastAPI Service
  â”œâ”€â”€ Input validation (Pydantic)
  â”œâ”€â”€ Model inference
  â”œâ”€â”€ Risk band mapping
  â”œâ”€â”€ Decision rules
  â”œâ”€â”€ SHAP explainability
  â””â”€â”€ Drift monitoring
        â”‚
        â””â”€â”€ Audit logs


ğŸš€ Live Deployment


Swagger UI:
ğŸ‘‰ http://3.106.188.194:8000/docs


Health Check:
GET /



ğŸ“¦ API Endpoints
1ï¸âƒ£ Predict Credit Risk
Endpoint
POST /predict

Request Payload (actual implementation)
{
  "num_late_payments": 2,
  "avg_delay": 12.5,
  "credit_utilisation": 0.42,
  "payment_ratio": 0.78,
  "high_risk_flag": 0,
  "LIMIT_BAL": 250000,
  "AGE": 34
}

Response
{
  "default_prediction": 0,
  "probability_of_default": 0.27,
  "risk_band": "MEDIUM",
  "decision": "REVIEW"
}


2ï¸âƒ£ Explain Prediction (SHAP)
Endpoint
POST /explain

Uses the same feature vector as inference and returns SHAP-based feature attributions explaining why the model predicted a given risk.
âœ” Ensures predictionâ€“explanation consistency

3ï¸âƒ£ Monitor Data Drift
Endpoint
POST /monitor/drift

Purpose


Compares incoming feature distributions against reference data


Detects statistically significant drift


Flags retraining recommendations


âš ï¸ Retraining is not auto-triggered by design â€” human approval is required.

4ï¸âƒ£ Retraining Readiness Check
Endpoint
POST /monitor/retraining-check

Returns a signal indicating whether recent drift patterns suggest that retraining should be considered.

ğŸ§¾ Audit & Traceability


Inference inputs and decisions are logged


Risk band mapping and decision rules are deterministic


Enables post-hoc inspection for:


Compliance


Debugging


Model behavior review




This mirrors real regulated ML systems (finance / risk).

ğŸ³ Containerization


Dockerized FastAPI service


Clean separation of:


Application code


Model artifact


Dependencies




docker build -t credit-risk-engine .
docker run -p 8000:8000 credit-risk-engine


â˜ï¸ Cloud Deployment


Hosted on AWS EC2 (Amazon Linux 2023)


Exposed via port 8000


Swagger UI publicly accessible



ğŸ” CI/CD (Engineering Reality)
GitHub Actions was used to experiment with EC2-based deployment automation.
This surfaced real-world issues including:


SSH connectivity timeouts


Remote Docker build context problems


Repository path mismatches on EC2


These challenges were intentionally not hidden, as they reflect common production deployment pitfalls.

ğŸ› ï¸ Tech Stack


Python


FastAPI


scikit-learn


SHAP


Docker


AWS EC2


GitHub Actions


Pydantic



ğŸ“‚ Repository Structure
credit-risk-engine/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ schemas.py
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ notebooks/
â”œâ”€â”€ data/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ credit_risk_pipeline.pkl
â””â”€â”€ README.md


ğŸ¯ What this project demonstrates


Serving ML models as real APIs


Explainability with SHAP


Drift detection in production settings


Audit-ready ML design


Cloud deployment with Docker


Practical CI/CD lessons (not toy pipelines)



